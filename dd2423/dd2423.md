# DD2423 Revision Notes

### Lecture 2

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.682.2930&rep=rep1&type=pdf

- Types of camera:
    - Non-perspective
        - Orthographic: 5 DOF
            - \begin{matrix}
              r_{00} & r_{01} & r_{02} & \Delta X \\
              r_{10} & r_{11} & r_{12} & \Delta Y \\
              0 & 0 & 0 & 1
              \end{matrix}
            - The z-axis is "discarded". We have no depth information. No idea how far object is, and moving the object further/closer makes no difference in size of projection.
            - Analogy: shadows when sun is overhead. Moving objects don't change size of shadows. Shadows are exactly the size of the object.
            - ![](https://i.imgur.com/rjAlzqA.png)
        - Scaled Orthographic: 6 DOF
            - \begin{matrix}
              r_{00} & r_{01} & r_{02} & \Delta X \\
              r_{10} & r_{11} & r_{12} & \Delta Y \\
              0 & 0 & 0 & Z_0
              \end{matrix}
            - Orthographic, then allow scaling proportionally. +1 DOF. Still no depth information.
            - Analogy: Scale to make closer objects larger.
            - ![](https://i.imgur.com/KEFs9YC.png)
            - Still no idea if boxes are same size, or one is larger and further.
        - Weak Perspective: 7 DOF
            - Insert matrix
            - Orthographic, but allow scaling differently in x- and y- directions, i.e. allow stretching. +2 DOF.
        - Affine Camera: 8 DOF
            - \begin{matrix}
              m_{00} & m_{01} & m_{02} & m_{03} \\
              m_{10} & m_{11} & m_{12} & m_{13} \\
              0 & 0 & 0 & 1
              \end{matrix}
            - Orthographic, then allow any affine transformation. +3 DOF.
    - Perspective
        - Pinhole: 9 DOF
            - Preserves perspective.
            - Assumes same focal length in x/y direction.
        - Generalization ("unknown aspect ratio"): 10 DOF
            - Pinhole, but allow change of focal length (or equivalently, odd aspect ratio pixels instead of square) in x/y direction separately. +1 DOF.
        - Projective: 11 DOF
            - \begin{matrix}
              m_{00} & m_{01} & m_{02} & m_{03} \\
              m_{10} & m_{11} & m_{12} & m_{13} \\
              m_{20} & m_{21} & m_{22} & m_{23} 
              \end{matrix}
            - Allow skew. +2 DOF.

- Types of transformations
    - Translation: 2 DOF
    - Euclidean: 3 DOF
        - \+ Rotation
    - Similarity: 4 DOF
        - \+ Scaling
    - Affine: 6 DOF
        - \+ Shear (rectangle -> parallelogram)
    - Projective: 8 DOF
        - \+ Foreshortening (rectangle -> trapezium)
        - Lines no longer need to remain paralllel
- Intrinsic/Extrinsic camera properties
    - Extrinsic
        - Irradiance (How much light in the surroundings)
        - Radiance (How much reflection from object)
        - World coordinates (Position), Rotation, Skew
    - Intrinsic ("Callibration")
        - Focal length fx, fy
        - Skew
        - Principal Point (x, y) often close to image center.
        - Projection matrix
        \begin{matrix}
          &   & f_x & \gamma & x_0 & 0 \\
        P & = &  0  &  f_y   & y_0 & 0 \\
          &   &  0  &    0   &  1  & 0 \\
        \end{matrix}
- Upscale/Transformation sampling
    - Example: we need to upscale an image. How do we fill in the blanks?
    - Nearest Neighbour sampling
        - Use colour of nearest neighbour.
        - Roughly preserves composition of colours.
        - Sharper but more noise ("amplifies" noise)
    - Bilinear Interpolation sampling
        - Use colour * distance to neighbour around neighbourhood.
        - Blurrer, but less noise ("blends" colours from surroundings.)

### Lecture 3

- Linear operators
    - Obeys
        - Scalar rule: Increase in strength of input increases strength of output
        - Additivity rule: Output consists of sum of individual responses to input.
    - Equivalently, satisfies
        - $\ell(\alpha f + \beta g) = \alpha \ell(f) + \beta \ell(g)$

- Shift-invariance
    - Obeys
        - Two idential input shifted by time produces two identical output shifted by the same amount of time
    - Equivalently, satisfies
        - $S(\ell(f)) = \ell(S(f))$ where S is shift and $l$ is linear operator.

- Convolution
    - ![](https://i.imgur.com/6NrdQOR.png)
    - Commutative
    - ![](https://i.imgur.com/IBfZjct.png)

- Input signal
    - Factorised into a summation of multiple delta functions.
        - $f(x) = 3 * \delta[x-0] + 4 * \delta[x-1] + 5 * \delta[x-2]$
        - ![](https://i.imgur.com/ILg9iR8.png)
    - Usually denoted with $x[n]$.
- Impulse-response
    - Is the output if a **single** *delta function* is fed into the system.
    - Represents a transformation done to a single input.
    - Also called a **kernel**
    - |![](https://i.imgur.com/HHPLAFw.png)| --> |![](https://i.imgur.com/2SlAZ1V.png)|
      |---|---|---|
- 1D Convolution
    - Feeding the entire input signal into a system with a pre-defined, linear shift-invariant system.
    - Equivalent to decomposing the input signal into separate delta functions, feeding them individually into the system, producing a known impulse-response, then summing up all the individual outputs.
    - ![](https://i.imgur.com/Lrob71M.png)
    - $[3\ 4\ 5] \ast [2\ 1]$
        - $[3] \ast [2\ 1] = [6\ 3]$
        - $[\_\ 4] \ast [2\ 1] = [\_\ 8\ 4]$
        - $[\_\ \_\ 5] \ast [2\ 1] = [\_\ \_\ 10\ 5]$
        - Sum it up to get
        - $[6\ 11\ 14\ 5]$

- 2D Convolution
    - Extend the 1D idea.
    - Each input produces a 2D impulse-response based on its own input strength.
    - Sum up the response due to all neighbours to produce individual output at a point.
    - ![](https://i.imgur.com/9cOktjX.png)
    - ![](https://i.imgur.com/no6NK0a.png)

- Convolution tricks
    - Invert the kernel, center the kernel on the input square desired, then simply take summation of pairwise multiplication.
    - For 2D kernels, flip both horizontally and vertically.
    - Separability: Separate a [NxN] kernel into a [Nx1]x[1xN] kernels. E.g.
    \begin{matrix}
    1 & 2 & 1 & & 1 & & & & & & & &\\
    2 & 4 & 2 &=& 2 & * & 1 & 2 & 1\\
    1 & 2 & 1 & & 1 & & & & & & & &\\
    \end{matrix}

- Fourier Transform
    - Convolution in spatial domain is equivalent to multiplication in fourier domain.
        - Speeds up calculation
    - Vice versa; Convolution in fourier domain is equivalent to multiplication in spatial domain.
    - Fourier transform converts spatial domain to fourier domain.
    - Continuous Fourier Transform
        - $$\hat{f}(\omega) = \int_{x \in \mathbb{R}^n} f(x)e^{-i\omega^Tx} dx$$
    - Discrete Fourier Transform
        - $$\hat{f}(u) = \frac{1}{\sqrt{M}^n} \sum_{x \in I^n} f(x)e^{-\frac{2\pi i u^T x}{M}}$$
        - $$\hat{f}(u, v) = \frac{1}{\sqrt{MN}} \sum_{x = 0}^N \sum_{y = 0}^M f(x, y)e^{-2\pi i(\frac{ux}{M} + \frac{vy}{N})}$$
    - Frequency variables in 1D related by $\omega = \frac{2\pi u}{M}$ where $u$ takes values $0...M$ and $\omega$ takes values $[0, 2\pi)$
    - Fourier transform results in each cell having a complex value. We define 
        - Magnitude as $|F(x, y)| = \sqrt{Re(x, y) + Im(x, y)}$
            - Is the peak value of the complex wave.
        - Phase as $\phi(x, y) = \tan^{-1}\frac{Im(x,y)}{Re(x,y)}$
            - Is the origin of the wave, or alternatively its phase.

- Properties of Fourier Transform
    - Separability
        - 2D FFT can be implemented as series of 1D FFT along each column, then 1D FFTs along each row.
        - ![](https://i.imgur.com/ymoaD29.png)
    - Linearity
        - Can operate on image before or after fourier transform and result will be the same.
    - Modulation
        - Multiplying original function $f(m,n)$ by $e^{2\pi i (\frac{m u_0}{M} + \frac{n v_0}{N})}$ shifts the origin from $(0,0)$ to $(u_0, v_0)$.
        - In particular, when $u_0 = \frac{M}{2}$ and $v_0 = \frac{N}{2}$, the multiplication becomes $(-1)^{m+n}$
        - *Multiplying original function $f(m,n)$ by $(-1)^{m+n}$ for each $m, n$ moves the origin to the center.*
    - Translation
        - If the image is moved, the magnitude of the fourier spectrum does not change, but its phase changes.
        - We can think of an image movement as a change in its starting position. Decomposing an image into its constituent frequencies, the sine wave remains the same, just translated. This corresponds to a change in phase but not its magnitude.
    - Rotation
        - Rotating the image rotates the fourier spectrum (both magnitude and phase) by the same angle.
    - Scaling
        - Compression in the spatial domain gives an expansion in the fourier domain, and vice versa.
        - If $f(t)=\hat{f}(\omega)$, then $f(kt)=\frac{1}{|k|}\hat{f}(\frac{\omega}{|k|})$ for a constant scaling factor k.
        - *When an image is compressed, we need to use a higher frequency to represent more fine/frequent changes in intensity*
    - Periodicity
        - The DFT and its inverse are periodic with period N for an NxN image.
        - *The image is "tiled" infinitely, and so is the $\hat{f}$ spectrum.*
    - Conjuate Symmetry
        - The fourier transform satisfies $\hat{f}(u, v) = \hat{f}^*(-u, -v)$. $\hat{f}$ has period N and conjugate symmetry around the origin.
        - ![](https://i.imgur.com/873V8oP.png)
        - We acually don't need to define the entire $2 * N^2$ for magnitude and phase (or equivalently real and imaginary), but just $N^2$ in total by exploiting symmetry.

- Transfer functions
    - Result of applying fourier transform to impulse-responses.
    - Spatial/Time domain | Fourier Domain
      --------------------|---------------
      Box Filter | Sinc function
      Gaussian | Gaussian
      Cosine | Real, 2x $\delta$ with $\omega = \omega_0$ and $-\omega_0$
      Sine | Imaginary, 2x $\delta$ with $\omega = \omega_0$ and $-\omega_0$
      Delta | Constant
      Constant | Delta with value $2 \pi$
      
- Sampling
    - Our world is continuous, but we process discrete events.
    - Sample with a collection of dirac functions.
        - Dirac function: Continuous, 1 at a certain point, 0 everywhere else. Integrate over function = 1.
        - *Continuous version fo Delta function*
    - Sources of error
        - Intensity quantization (not enough intensity resolution)
        - Spatial aliasing (not enough spatial resolution)
        - Temporal aliasing (not enough temporal resolution)
    - How to reduce aliasing
        - Subsample, then blur: cause blurred artifacts.
        - Blur, then subsample: reduce amount of information required, then subsample to achieve lesser aliasing.
        - Increase sampling rate...
    - Sampling theorem
        - If the signal is sampled at a rate equal or greater to than twice its highest frequency, the original signal can be completely recovered from it samples (Shannon).
        - The minimum sampling rate to achieve this lossless sampling is called Nyquist rate.

### Lecture 5
- Gray-level transformations
    - Look-up Table
        - Convert pixel values according to a pre-defined look-up table (e.g. [1-5] -> 1, [5.0-5.5] -> 2, [5.5-6.0] -> 3)
    - Image negative
        - $output = 2^k - 1 - input$ where k is max no. of bits.
    - Log Transformation
        - $output = c \log{(1+input)}$
        - We used this to reduce range of our fourier spectrum.
    - Power-law / Gamma correction
        - $output = c (input)^\gamma$
    - Histogram stretching
        - To allow a middle section of histogram replace the entire gray-value range. Note: information loss in sections a-c and d-b.
        - ![](https://i.imgur.com/SuptkOf.png)
        - Equivalent to a hard-stretching, a non-monotonic function that is not invertible.
        - ![](https://i.imgur.com/qkHG0YF.png)
    - Histogram equalization
        - An automatic way to redistribute gray levels evenly. (A brightness distribution where all levels are equally likely).
        - Look for a transformation $s = T(r)$ such that $\rho_s(s)$ is uniformly distributed given image $\rho_r(r)$.
        - Continuous case
            - For a section $dr$ of original image, number of pixels = $\rho_r(r) \cdot dr$.
            - These should map to a new section $ds$.
            - $\rho_s(s) \cdot ds = \rho_r(r) \cdot dr$
            - Note that $rho_s(s) = 1$.
            - So $ds$ (new pixel range) = $\rho_r(r) \cdot dr$.
            - $s = T(r) = \int_0^r \rho_r(w) dw$
        - Discrete case
            - Total number of pixels = N.
            - $s_k = \sum_{i=0}^k \frac{n_i}{N}$ for $k = 0, 1, ..., 255$
- Spatial filtering
    - Lowpass
        - Net blurring effect
        - Ideal lowpass: remove all frequencies > $f_0$
            - Causes ringing effect. Box filter in frequency domain = sinc filter in spatial domain.
            - Sharp edges (square waveforms) are combinations of many frequencies
            - ![](https://i.imgur.com/5Q52DDk.png)
        - Gaussian lowpass
            - Gaussian in frequency domain -> still gaussian in spatial domain.
            - Wide filter in spatial domain <-> narrow filter in frequency domain
        - Binomial kernel
            - \begin{matrix}
              \frac{\Delta t}{2} &   &     &     &     \\
              1 - \Delta t & * & \frac{\Delta t}{2} & 1 - \Delta t & \frac{\Delta t}{2} \\
              \frac{\Delta t}{2} &   &     &     &     \\
              \end{matrix}
            - $\Delta t = \frac{1}{2}$ gives
              \begin{matrix}
              1/16 & 1/8 & 1/16 \\
              1/8 & 1/4 & 1/8 \\
              1/16 & 1/8 & 1/16 \\
              \end{matrix}
            - Central limit theorem: approaches gaussian.
    - Highpass
        - Net sharpening effect, add noise.
    - Bandpass
        - Combination of both
- Noise
    - Signal dependent additive
        - g = f + v
    - Signal dependent multiplicative
        - g = f + vf
    - Measurement noise / salt-and-pepper

- Filters: local spatial averaging
    - Mean filter
        - \begin{matrix}
          1/9 & 1/9 & 1/9 \\
          1/9 & 1/9 & 1/9 \\
          1/9 & 1/9 & 1/9 \\
          \end{matrix}
        - Errors are spread across the image
        - Tends to blur edges
    - Binomial filter
- Filters: non-linear
    - Incorporate prior knowledge, instead of having pre-defined / uniform filter masks.
    - Avoid destructive behaviour near edges
        - Median filter
            - Preserves position of 1D edges
            - Eliminates local extremes like salt-and-pepper
            - Creates painting-like images
        - Selective/weighted averaging
            - Only smooth with neighbours of similar colour / smooth with weight = similarity.

- Filters: sharpening
    - Unsharp masking: subtract the blur.
    - High-pass filter
        - Similar to unsharp masking but in frequency domain.


- Derivative filters
    - First order:
        - Zero in flat areas
        - Non-zero in areas of change
        - By definition: $f_x = f(x+1, y) - f(x, y)$
        - More common: $f_x = \frac{1}{2} * (f(x+1, y) - f(x-1, y))$
            - = $[\frac{1}{2}\ 0\ -\frac{1}{2}]$
    - Second order:
        - Zero in flat area / area of constant change
        - Non-zero near onsets of change.
        - $f_{xx} = f(x+1, y) - 2f(x, y) + f(x-1, y)$
    - Laplacian operator:
        - Isotropic (rotation invariant)
        - $\nabla^2f = f_{xx} + f_{yy}$

### Lecture 6
- Hough transform
    - Detects lines spanned by edges.
    - Point in $(x,y)$ space mapped to $(\theta, r)$ space.
        - $r(\theta) = x \cos \theta + y \sin \theta$
    - For each edge $(x, y)$,
        - Loop each discrete value of $\theta \in [-\frac{\pi}{2}, \frac{\pi}{2}]$, then calculate corresponding value of r. $acc[\theta][r]$ += $f(x)$.
            - $f(x) can be 1 or point (edge) intensity.
            - Edges due to noise likely to have lower intensity. Give more weightage to true edges. Can also use thresholding to remove noisy edges first.
            - Can also increment theta and r within a gaussian window. Equivalent to smoothing after voting.
    - Effect is a curve. https://www.youtube.com/watch?v=4zHbI-fFIlI
    - $\theta$ and $r$ too coarse: poor resolution in line directions. Too fine: not enough samples --> repeated lines.
    - What about circles? Can we detect circles?
        - Yes. If we assume circles with fixed, known radius r, then we effectively draw a circle in Hough domain for each edge point. 
        - If we don't know the radius, then we have to bring this to 3D plane and draw radii $0$ to $N$ for each circle, where N is image size.

- Homographies
    - https://www.youtube.com/watch?v=bRyUvt0ZnU0
    - https://www.youtube.com/watch?v=MlaIWymLCD8
    - Given two camera images of the same world, we want to know how the cameras are related to each other. This will then allow us to compute transformations from one image to another.
    - We detect point features (SIFT/SURF), then match them. If we have more data points than necessary, we obtain H by solving least-squares. 
        - For camera 1,
        - ![](https://i.imgur.com/yzGWR1X.png)
        - where $x_i, y_i$ are image coordinates and $X_i, Y_i, Z_i$ are world coordinates. The matrix p is a projection matrix comprising [3x4] intrinsic and [4x4] extrinsic properties. p translates from world coordinates to image coordinates.
        - Normally, this matrix is not invertible. However, since we are free to define the world coordinates, we can define such that $Z_i = 0$. Then we can throw out column 2 to get ![](https://i.imgur.com/OqNYZC9.png) which is usually invertible.
        - Then $[wx_i\ wy_i\ w]^T = H_1[X_i\ Y_i\ 1]^T$.
        - Equivalently, $[X_i\ Y_i\ 1]^T = H_1^{-1}[wx_i\ wy_i\ w]^T = H_2^{-1}[\omega' x'_i\ \omega' y'_i\ \omega']^T$
        - So $[wx_i\ wy_i\ w]^T = H_1[X_i\ Y_i\ 1]^T = H_1 H_2^{-1}[\omega' x'_i\ \omega' y'_i\ \omega']^T$
        - We define $H = H_1 H_2^{-1}$.
        - It's a mapping from image 1 -> World -> image 2.
        - So, now that we have such a mapping, how do we find this [3x3] H exactly?
        - H has 8 degrees of freedom ($p_{23}$ is a scale factor), so we need 8 equations or 4 data points (a pair of $x_i,y_i$ each) to solve for H.
        - As an example, $x_i = \frac{wx_i}{w} = \frac{h_{00} x'_i + h_{01} y'_i + h_{02}\omega'}{h_{20}x'_i + h_{21} y'_i + h_{22}\omega'}$.
        - We make $\omega' = 1$ (this is fine - h will adjust accordingly) to form an simultaneous equation involving H only.
        - ![](https://i.imgur.com/t8YY0lq.png)
        - We will solve least squares: $(A^TA)^{-1}A^Th$

- Shape description
    - We can describe a shape with
        - Pixel values | list of features
        - Boundary | Region
        - Local | Global
        - Geometric | Statistical
        - Rigidity / Deformable
        - Completeness
        - Variance under transformation
    - To make our description robust (i.e. no variance under transformation), we need
        - Translation invariance
        - Scale invariance
        - Rotation invariance (6 vs 9)
        - Reflection invariance
    - Some descriptors
        - Size (# pixels)
        - Bounding box pixel location
        - Centre of gravity
        - Compactness (area / circumference^2)
        - Eccentricity (length of maximum chord / max perpendicular chord)
    - **Moment descriptor of region**
        - Describes statistical features like mean, variance, centre of gravity,
        - $m_{pq} = \sum_{x,y}x^p\cdot y^q \cdot f(x,y)$
        - Centered moment $\mu_{pq} = \sum_{x,y}(x-\bar{x})^p\cdot (y-\bar{y})^q \cdot f(x,y)$
        - Where $\bar{x}, \bar{y}$ is centre of gravity $\bar{x} = \frac{m_{10}}{m_{00}}, \bar{y} = \frac{m_{01}}{m_{00}}$
        - ![](https://i.imgur.com/RzTFr3X.png)
- PCA
    - Reduce dimensionality
    1. Compute mean
        - $\bar{x} = \frac{1}{M}\sum_{i=1}^M x_i$
    2. Shift mean to centre / subtract mean
        - $\phi_i = x_i - \bar{x}$
    3. From matrix $A = [\phi_1, \phi_2, ..., \phi_M]$, compute covariance
        - $C = A\cdot A^T$
    4. Compute eigenvalue of C
        - Find $\lambda$ such that $|C-\lambda I| = 0$
    5. Order eigenvalues in decreasing value $\lambda_1, \lambda_2, ... \lambda_M$.
        - $PC_i$ = Compute $[C-\lambda_i I | 0]$
    6. Keep only the largest K lambdas.

### Lecture 7
- Scale-space theory
    - Represent image data at all scales simultaneously.
    - Expand the dataset into another dimension of scale.
- Properties of scale representations
    - Linearity
        - $T(af_1 + bf_2) = aTf_1 + bTf_2$
    - Shift invariance
        - T(S(f)) = S(T(f))
        - where S is the linear shift operator.
    - Semi-group structure
        - $T_{S_1}T_{S_2} = T_{S_1+S_2}$
    - Scale covariance
        - $T_{S(s)} S f = S T_S f$
    - Non-creation of structure with increaing scale
        - If, originally $x_0$ is a local maxima / minima, the gradient of $T_s(x)$ should be $\leq 0$ / $\geq 0$ respectively.
        - Can only reduce peaks, not intensify them.
    - Must satisfy ![](https://i.imgur.com/dVUUvxc.png)
        - $g(x; s) = \frac{1}{2\pi s}e^{-\frac{x^Tx}{2s}} = \frac{1}{2\pi s}e^{-\frac{x_1^2 + x_2^2}{2s}}$
        - Objects smaller than sqrt(s) pixels will become blurred together.
- Derivative of Gaussian operators
    - Combine differentiation with gaussian (blur).
    - Because $∂(f*G_s)=f*∂G_s$

- Edge detection
    - Assumption: A discontinuity in image brightness corresponds to a discontinuity in depth/orientation/reflection/illumination.
    - Our job, when identifying edges, is to detect these discontinuities
    - Use 2nd order derivative to detect regions of sharp intensity change.
    - Problem: 2nd order derivative is very sensitive to noise.
            - e.g. arctan function. $\frac{d}{dx} = \frac{1}{1+x^2} = \infty$ near $x = 0$.
        - Solution: smooth image first.
        - We need to remember that more smoothing causes more distortion of "true" structures, although too little smoothing results in too much noise.
        - Actually... Instead of blurring image, then applying differential operators, we can merge them together! We can make a 2nd order differential gaussian filter. 
        - Solution: apply thresholding
            - Not all transitions are edges. Some are just slight or gradual changes in intensity.
            - Only consider the transitions where Lx or Ly > threshold.
- Laplacian edge detection
    - 1D: Edges are peaks in first-order derivatives, and zero-crossings in second-order derivatives.
    - 2D: Same, but for second-order derivative, we can use Laplacian operator $\nabla^2L = L_{xx} +L_{yy}$ as a rotationally symmetric operator for the 2nd order derivative 
        - i.e. find zero-crossings of Laplacian operator.
        - ![](https://i.imgur.com/rKOFMNR.png)
    - Kernel:
        - $\begin{matrix}
          0 & 1 & 0 \\
          1 & -4 & 1 \\
          0 & 1 & 0
          \end{matrix}$
    - Problem: This edge detection is very sensitive to noise. How? 
        - As mentioned above, blur first.
    - Problem: Zero-crossings will also respond to false edges: minimas in 1st order derivative.
        - We need to add a check to ensure 1st order is not negative. Alternatively, use gradient based edge detection.
- Gradient based edge detection
    - Lx = convolve with $\begin{matrix}
                         0 & 0 & 0 \\
                         1/2 & 0 & -1/2 \\
                         0 & 0 & 0
                         \end{matrix}$
    - Ly = convolve with $\begin{matrix}
                         0 & 1/2 & 0 \\
                         0 & 0 & 0 \\
                         0 & -1/2 & 0
                         \end{matrix}$
    - Similar to prewitt operator. Remember to invert kernel.
    - Gradient direction: $\tan^{-1}\frac{L_y}{L_x}$.
- Why do we use $\frac{1}{2}[1\ 0\ -1]$ as differential kernel?
    - Taylor expansion:
        - $f(x+h) = f(x) + hf'(x) + \frac{1}{2} h^2f''(x) + ...$
        - $f(x-h) = f(x) - hf'(x) + \frac{1}{2} h^2f''(x) + ...$
        - $f'(x) = \frac{f(x+h) - f(x-h)}{2h}$.

- Canny edge detector
    - When using edge detectors that rely on 1st-order differential operators, we get thick edges if we set a low threshold (however, we risk losing edges if we set a high threshold) on images that have more gradual changes in intensity.
    - How do we reduce the thickness of these edges?
        - Canny edge detector can reduce them to 1 pixel thick.
        - For each edge identified, first find its direction from $\theta = tan^{-1}\frac{L_y}{L_x}$. Along the edge, keep only the pixel of maximal intensity. 
    - How do we set the threshold?
        - Canny edge detector uses Hysteresis Thresholding.
        - Determine 2 thresholds $T_{high}$ and $T_{low}$. Edges with intensity $\geq T_{high}$ are kept, $\leq T_{low}$ are discarded, and those in between the thresholds are kept if they are connected to an edge that is kept.

- Lindeberg differential edge detection
    - Similar idea to Canny edge detector, but express gradient in direction of edge with directional derivative.
    - Given $L$, $L_x$ and $L_y$, we let v be direction of edge.
    - $L_v = \delta_v L$ where $\delta_v = \cos v \cdot \delta_x + \sin v \cdot \delta_y = \frac{L_x}{\sqrt{L_x^2 + L_y^2}} \cdot \delta_x + \frac{L_y}{\sqrt{L_x^2 + L_y^2}} \cdot \delta_y$.
    - We can check that $L_v = \delta_v L = \frac{L_x}{\sqrt{L_x^2 + L_y^2}} \cdot \delta_x L + \frac{L_y}{\sqrt{L_x^2 + L_y^2}} \cdot \delta_y L = \frac{L_x^2}{\sqrt{L_x^2 + L_y^2}} + \frac{L_y^2}{\sqrt{L_x^2 + L_y^2}} = \sqrt{L_x^2 + L_y^2}$ 
    - ![](https://i.imgur.com/TgzYYAl.png)
    - $L_{vv} = \frac{\delta_v^2 L = L_x^2 L_{xx} + 2L_xL_yL_{xy} + L_y^2L_{yy}}{L_x^2 + L_y^2}$
    - Avoids issues of orientation estimation and handling as well as edge tracking in discrete non-maximum suppression

### Lecture 7
- Important filters
    - Gradient approximation kernels
        - Operators given below are for x-direction.
        - Sobel Operator: \begin{matrix}
                          1 & 0 & -1 \\
                          2 & 0 & -2 \\
                          1 & 0 & -1 \\
                          \end{matrix}
        - Prewitt Operator: \begin{matrix}
                            1 & 0 & -1 \\
                            1 & 0 & -1 \\
                            1 & 0 & -1 \\
                            \end{matrix}
                            
                            
### Lecture 8
- Feature matching
    - Good interest points should
        - have clear well-defined foundation
        - have well-defined position in image
        - be rich in information content
        - be stable under transformations
        - be distinct from other points
        - have an attribute of scale
- Harris corner detection
    - https://www.youtube.com/watch?v=WyrVzTRZuXA
    - Compute partial derivatives $L_x$ and $L_y$ at some scale t.
    - Compute $L_x^2$, $L_xL_y$ and $L_y^2$ at every image point.
    - Compute $E(L_x^2)$, $E(L_xL_y)$ and $E(L_y^2)$, the respective sum over a local window.
    - Derive $\mu$, a symmetrical 2nd-moment matrix that looks like
    - \begin{matrix}
      E(L_x^2) & E(Lxy) \\
      E(Lxy) & E(L_y^2) \\
      \end{matrix}
    - We can derive its eigenvalues $\lambda_1, \lambda_2$, both of which will be $\geq 0$.
        - If both $\lambda_1$ and $\lambda_2$ are small, then it's flat region
        - If either $\lambda_1$ or $\lambda_2$ is significantly large, then it's edge
        - If both $\lambda_1$ and $\lambda_2$ are large, then it's corner.
    - Eigenvalues are expensive to compute, so we can also compute
        - $H = \det \mu - k (\text{trace}\ \mu)^2$
        - With preferred k = [0.04, 0.06]
            - A larger k = more restrictive detector
        - If $H > H_0$, a pre-defined threshold (i.e. H is large), then it's corner.
        - If $H ~= 0$, then it's flat region
        - If $H << 0$, then it's edge.

- Laplacian blob detection
    - Given a scale-space representation L of an image f obtained through gaussian smoothing,
    - Compute the Laplacian operator
        - $\nabla^2L = L_{xx} + L_{yy}$
    - $\nabla^2L < 0$: "bright blob"
    - $\nabla^2L > 0$: "dark blob"
    - Responds when there is strong response in either direction (i.e. large $L_{xx}$ or $L_{yy}$) due to addition
    
- Hessian blob detection
    - Given a scale-space representation L of an image f obtained through gaussian smoothing,
    - Compute the determinant of hessian matrix
        - HL = \begin{matrix}
               Lxx Lxy
               Lxy Lyy
               \end{matrix}
        - $\det HL = L_{xx} L_{yy} - L_{xy}^2$
    - $\det HL < 0$: "bright blob"
    - $\det HL > 0$: "dark blob"
    - Responds only when there is strong response in both directions (i.e. large $L_{xx}$ and $L_{yy}$) due to multiplication
    - More accurate than Laplacian, better properties under affine deformations.
    - Both Laplacian and Hessian are better than Harris-Laplace.

- Complementary thresholding
    - Combine both
    - Compute
        - $D_1L = L_{xx} L_{yy} - L_{xy}^2 - k(L_{xx} + L_{yy})^2 \geq 0$
        - Effectively $\text{Hessian} - k(\text{Laplacian})^2$.
        - for preferable k = [0.04, 0.08]
    - This works because, we divide by $L_{yy}^2$. Order such that L_{yy} > L_{xx} > 0, then this value will be positive only if ratio $\frac{L_{xx}{L_{yy}}$ is close to 1.

- Scale-space extrema
    - Detect points $(\hat{x}, \hat{y}, \hat{t}) in scale-space that are extrema of both local space and scale.
    - These points are known as scale-space extrema.
    - Scale-normalized Laplacian
        - $\nabla_{\text{norm}}^2L = t(L_{xx} + L_{yy})$
    - Scale-normalized det of Hessian
        - $\nabla_{\text{norm}}^2L = t^2(L_{xx} L_{yy} - L_{xy}^2)$
    - Using these normalized equations, detect local maxima in scale by comparing across a 3x3x3 neighbourhood (x, y, t).

- SIFT
    - Detect keypoints
        - Detect scale-space extrema. Should use Laplacian of Gaussian (per above), but Difference of Gaussian is a more efficient approximation.
    - Compute gradients: $\nabla L (=L_x+L_y)$ around scale $\hat{t}$ of interest point
    - Orientation normalization of these detected blocks through detecting peaks in orientation histogram
    - Represent interest point with 4x4 grid, each accumulating gradient direction in 8 directions.
    - Shift-invariant, rotation invariant, scale invariant, but not affine-invariant.
- SURF
    - Detect keypoints
        - Use normalised determinant of Hessian
    - Compute gradients: partial derivatives $L_x$ and $L_y$ around scale $\hat{t}$ of interest point
        - Method: Approximate derivatives with box filters (Haar wavelets). This is faster due to ability to pre-compute. However, sacrifies rotational invariance, scale-space properties, and causes ringing phenomena
    - Orientation normalization similar to SIFT
    - Sum derivatives $\sum{L_x}, \sum{|L_x|}, \sum{L_y}, \sum{|L_y|}$ around 4x4 grid.
    - Faster, however less accurate than SIFT.

- Test SIFT vs SURF
    - Take standardised images of scale and skew.
    - Transform one image to another using homography, scale interest points in size accordingly.
    - Represent interest points detected by algorithms with circles of size proportionate to scale.
    - Accept a match if $\frac{\text{intersection}(\text{circle})}{\text{union}(\text{circle})} > \text{threshold} = 0.9$

### Lecture 9
Image segmentation with
- Automatic Thresholding
    - Assumption: Object of interest is brighter than background.
    - P-Tile method
        - Set threshold H using prior knowledge about p% of image being object.
            - However, object may not be lowest/highest p% of image intensity.
            - If we know x% of image is object, then we have identified the object already! Chicken and egg problem.
    - Mode method
        - Given histogram, segment image at largest valley.
            - But how to determine "largest" valley? It's not simply the smallest value.
                - Maximise "peakiness": difference between immediate peak and valley.
            - Object also need not lie on either side of split.
            - ![](https://i.imgur.com/HxgmmWk.png)
    - Iterative thresholding
        - Define correctness/goodness measure T for segmentation.
            - e.g. even split
        - Guess an approximate threshold and binary search.
    - Adaptive thresholding
        - In case of uneven illumination, split into m x m subimages and threshold each subimage.
- K-means clustering
    - Algorithm
        - Choose K initial colours.
        - For each pixel, choose nearest colour.
        - Take average of colours in each group.
        - Repeat till convergence
    - Problem
        - Sensitive to initial colour choice
        - May converge to local optima
        - Don't know how many initial colours to choose
- Gaussian Mixture Model
    - Similar to K-means, but instead of assuming that each pixel can only belong to 1 group, assume that it's a weighted % of multiple groups, defined by a gaussian.
    - Colour distribution of pixel i is
        - $P(C_i) = \sum_{k}P(z_i=k)P(c_i|z_i=k) = \sum_{k}\pi_k \mathcal{N}(c_i; \mu_k, \Sigma_k)$
        - $\pi_k = P(z_i = k)$ is probability that a pixel belongs to class k = $\frac{\text{pixels_in_k}}{\text{total_pixels}} and $\mathcal{N}(c_i; \mu_k, \Sigma_k)$ is probability of pixel i being that class, given its distribution.
    - Use expectation-maximization to converge.
    - Allows clusters to be elliptic. K-means assumes round clusters.
- Spatial coherence
    - We need to take into account (x,y) data also because neighbouring pixels (of similar colour) is likely the same object.
- Mean-shift segmentation
    - Group pixels in both colour and position
    - Each pixel represented by (x, y, R, G, B).
    - For each pixel, create a gaussian "sphere" around it, and multiply other pixels by this gaussian function.
    - This creates a weighted mode with penalisation by distance. Adjusting the $\sigma$ values of the gaussian function changes the tolerance for such distance, and is known as the bandwidth. A larger $\sigma$ means larger bandwidth that is more tolerant to changes in that feature.
    - Use the update formula $\hat{x} \leftarrow \frac{\sum_{i=1}^n \hat{x_i} K(\hat{x} - \hat{x_i})}{\sum_{i=1}^n K(\hat{x} - \hat{x_i})}$.
        - $\hat{x_i}$ is pixel point
        - $\hat{x} is weighted centre.
    - We will group pixels that converge to the same place.
- Merging and Splitting
    - Over-segment first, then merge segments that are similar
        - In mean intensities
        - Have better statistical distributions (closer to a priori knowledge)
        - Weakness of boundary (similarity of intensities of the two sides)
- Watersheddng
    - ![](https://i.imgur.com/WRt7JoK.png)
    - Usually leads to over-segmentation
- Graph Theory
    - Represent pixels as nodes and neighbours as edges. Weight of neighbour = similarity.
- Normalized Cuts
    - Mnimise $\text{NCut(A, B)} = \frac{\text{cut(A, B)}}{\text{assoc(a, v)}} + \frac{\text{cut(A, B)}}{\text{assoc(b, v)}}$.
        - cut(A, B) = sum of all edges that link between A and B.
        - assoc(A, V) = sum of all edges that terminate (at least one side) in A.
        - Effectively, minimise cross-cluster similarities, maximise within-cluster similarities.
        - Denominator present to ensure cut is rather even (instead of one tiny and one large)
    - Minimizing NCut = solving $min_y\frac{y^T (D-W)y}{y^T Dy}$ = solving $(D-W)y = \lambda Dy$ = solving $(I-D^{1/2}WD^{-1/2})z = \lambda z$; where $z = D^{1/2}y$

### Lecture 10
- Snake & Active Contour
    - Represent shape by an elastic rubber band curve
    - Minimize $\sum$ internal and external energy
        - Internal energy:
            - $\int \alpha(s) ||X_s(s)||^2 + \beta(s) ||X_{ss}(s)||^2 ds$
            - First term makes curve short, not "stretchy". Each point along the curve evenly spread out.
            - Second term makes curve smooth. Reduce small bumps in favour of smooth curve.
        - External energy:
            - $-\int ||\nabla I(X(s))||^2 ds$ = $-\int ||\frac{\delta I}{\delta x}(x(s), y(s)) + \frac{\delta I}{\delta y}(x(s), y(s))||^2 ds$
            - If there is no edge in image at $X(s) = (x_s, y_s)$, then external energy = 0.
            - If there is presence of edge, external energy highly negative. Wrapping the curve around this edge provides incentive for strong reduction in energy --> curve will tend to wrap around this edge.
        - Can also add whatever energy suitable for task, e.g. must pass through some anchors.
        - s is where we are on the curve, 0 = start and 1 = end.
    - Iteratively move along normal to decrease energy.

- IDK Level Set
- GrabCut
    - Create a colour model, or have the user input a colour model (what is in foreground, and what is in background)
    - Label each pixel with a cost: low if it's similar to model $I_x$, high otherwise.
    - Add costs for links: between neighbouring pixels. Low/zero cost if pixels have different colour, high cost for similar colour.
    - Result: pixels aligned to most similar models, while image is cut along most contrasting models - edges.
    - Effectively split the graph into two with lowest energy cost.
    - Min-cut = Max-Flow problem.

- Mathematical Morphology
    - Need to clean up segmentation.
    - Given a kernel, "convolve" the kernel with the image, but ignore any 0s in the kernel.
    - Dilation
        - If at least 1 pixel in the structuring element / kernel coincides with the foreground element (1), then result is 1. Pad with 0s, so borders don't affect image.
    - Erosion
        - If at least 1 pixel in the structuring element does not coincide with foreground elment, then result is 0.
        - Pad with 0s, so borders will all turn black (0).
    - Opening
        - Erosion then dilation
    - Closing
        - Dilation then erosion.
        - CLOSE up the holes

### Lecture 11

Stages of identifying items
- Recognition: Is this my cup?
    - Requires model of only 1 particular cup.
- Classification: What is this?
    - Requires generic idea of cup.
- Detection: Is there a cup? If so, where is it?
    - Requires cup that stands out from non-cups
- Need
    - Hypotheses: Cup or not
    - Representation: What defines a cup? needs to be sucinct.
    - Metric: How to score something that is similar to a cup

Object matching should be invariant to
- Lighting conditions
- Poses (viewing conditions)
- Deformations (changes in shapes)
- Clutter (blockages)
- Projective sizes (distances)
- Cameras (projections, distortions)

Represent objects with
- Template based (dense)
    - An image of object. Normalize lighting/scale to attain invariance.
- Feature based (sparse)
    - Store sets of characteristic features (e.g. corners)
    - Relative positions flexible
- Statistics based (usually dense)
    - Store histograms from data
    - Positions disregarded
    - Each type of data matched invariantly

Methods contain
- Feature detector
- (Invariant) descriptors for these features
- A way of combining features
    - Usually w.r.t position.

Matching with SIFT
- Match features between model and object.
- Remove mismatches using homography (ensure not apple-orange comparison)
    - If enough features are matched in both, then YES.

Matching with Bag of Words
- Scales well
- Represent image as histogram of features.
- Algorithm
    - Feature extraction from training images (using SIFT or otherwise)
    - Codebook creation
        - Represent each feature as a coordinate in feature space (e.g. sift = 128D space)
        - Cluster features using K-Means or Mean-shift
            - Each cluster centre is known as a "word" / vocabulary
    - For each object, collect histogram of features.
    - Classify histogram into different object classes.

Which class does a feature vector belong to?
- Nearest neighbour / k-NN
    - + Works well for well-separated classes
    - + Can represent clusters using complex shapes
    - - Doesn't scale in Dimension
    - - Depends on choice of metrics
    - - Sensitive to outliers (fixed with k-NN)
- Bayesian classification
    - $p(k_i | z) = \frac{p(z|k_i)p(k_i)}{\sum_{x\in K} p(z|k_x)p(k_x)}$
    - $p(k_i)$ = Prior probablity of this class $k_i$ happening.
    - $p(z|k_i)$ = Probability of z being in class $k_i$ given that we know the distribution of $k_i$.
    - Denominator common.
    - Decision boundary $p(z|k_1)p(k_1) = p(z|k_2)p(k_2)$.
- Discriminant function
    - Fixed discriminant function (e.g. linear y = mx+c)
    - Find best way to split the data into two sides.
    - e.g. use SVM / Boosting.

### Lecture 12
Stereo geometry
- Verging cameras
    - ![](https://i.imgur.com/JcjAGed.png)
    - Vergence angle (between principal directions) $2\mu = p_L - p_R$
    - Gaze angle (angle that cyclopean eye needs to turn) $\gamma = \frac{1}{2}(p_L + p_R)$
- Parallel cameras
    - $x_L = f\frac{X}{Z}$
    - Camera moves by $T_x =$ object move by $-T_x$.
    - $x_R = f\frac{X-T_x}{Z}$
    - $d= x_L - x_R = f\frac{T_x}{Z}$.
    - $T_x$ known as the **stereo baseline**.
    - $d = x_L - x_R$ known as **disparity**.
    - $Z = f\frac{T_x}{d}$.
    - Cyclopean system we take centre as origin.
    - $x_L = f_L \frac{X + T/2}{Z}$, $x_R = f_R\frac{X-T/2}{Z}$. Note the direction - camera moves right so object moves left.
    - We can derive $X = \frac{T}{2} \frac{x_L + x_R}{x_L - x_R}$! With data from 2 cameras $x_L$ and $x_R$ we can derive object location!
    - We can also derive $Z = \frac{Tf}{x_R - x_L}$. Depth Z is inversely proportionate to disparity 
- Wider baseline
    - Gives more accurate reconstruction ($\frac{\delta Z}{\delta d} = -\frac{Tf}{d^2} = -\frac{Z^2}{Tf}$ is smaller as T increases.)
    - However harder matching problem due to larger perspective distortion and larger proportion of occlusion.

- Epipolar Geometry
    - Cameras no longer parallel
    - Accomplished with translation + rotation